#!/usr/bin/env python
"""
æ‰‹åŠ¨æµ‹è¯• Lambda å’Œé›†åˆæ“ä½œåŠŸèƒ½
"""

from src.flowby.lexer import Lexer
from src.flowby.parser import Parser
from src.flowby.interpreter import Interpreter
from src.flowby.context import ExecutionContext


def test_lambda_single_param():
    """æµ‹è¯•å•å‚æ•° Lambda"""
    print("\n=== æµ‹è¯•å•å‚æ•° Lambda ===")
    source = """
let double = x => x * 2
let result = double(5)
"""
    tokens = Lexer().tokenize(source)
    ast = Parser().parse(tokens)
    context = ExecutionContext('test-task')
    interpreter = Interpreter(context)
    interpreter.execute(ast)

    result = context.symbol_table.get("result", line_number=0)
    print(f"âœ… result = {result}, æœŸæœ› = 10")
    assert result == 10


def test_filter_basic():
    """æµ‹è¯• filter æ–¹æ³•"""
    print("\n=== æµ‹è¯• filter æ–¹æ³• ===")
    source = """
let numbers = [1, 2, 3, 4, 5, 6]
let evens = numbers.filter(x => x % 2 == 0)
"""
    tokens = Lexer().tokenize(source)
    ast = Parser().parse(tokens)
    context = ExecutionContext('test-task')
    interpreter = Interpreter(context)
    interpreter.execute(ast)

    evens = context.symbol_table.get("evens", line_number=0)
    print(f"âœ… evens = {evens}, æœŸæœ› = [2, 4, 6]")
    assert evens == [2, 4, 6]


def test_map_basic():
    """æµ‹è¯• map æ–¹æ³•"""
    print("\n=== æµ‹è¯• map æ–¹æ³• ===")
    source = """
let numbers = [1, 2, 3, 4]
let doubled = numbers.map(x => x * 2)
"""
    tokens = Lexer().tokenize(source)
    ast = Parser().parse(tokens)
    context = ExecutionContext('test-task')
    interpreter = Interpreter(context)
    interpreter.execute(ast)

    doubled = context.symbol_table.get("doubled", line_number=0)
    print(f"âœ… doubled = {doubled}, æœŸæœ› = [2, 4, 6, 8]")
    assert doubled == [2, 4, 6, 8]


def test_reduce_sum():
    """æµ‹è¯• reduce æ±‚å’Œ"""
    print("\n=== æµ‹è¯• reduce æ±‚å’Œ ===")
    source = """
let numbers = [1, 2, 3, 4, 5]
let sum = numbers.reduce((acc, x) => acc + x, 0)
"""
    tokens = Lexer().tokenize(source)
    ast = Parser().parse(tokens)
    context = ExecutionContext('test-task')
    interpreter = Interpreter(context)
    interpreter.execute(ast)

    sum_result = context.symbol_table.get("sum", line_number=0)
    print(f"âœ… sum = {sum_result}, æœŸæœ› = 15")
    assert sum_result == 15


def test_chained_operations():
    """æµ‹è¯•é“¾å¼è°ƒç”¨"""
    print("\n=== æµ‹è¯•é“¾å¼è°ƒç”¨ ===")
    source = """
let numbers = [1, 2, 3, 4, 5, 6]
let result = numbers.filter(x => x % 2 == 0).map(x => x * 2)
"""
    tokens = Lexer().tokenize(source)
    ast = Parser().parse(tokens)
    context = ExecutionContext('test-task')
    interpreter = Interpreter(context)
    interpreter.execute(ast)

    result = context.symbol_table.get("result", line_number=0)
    print(f"âœ… result = {result}, æœŸæœ› = [4, 8, 12]")
    assert result == [4, 8, 12]


if __name__ == "__main__":
    try:
        test_lambda_single_param()
        test_filter_basic()
        test_map_basic()
        test_reduce_sum()
        test_chained_operations()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
